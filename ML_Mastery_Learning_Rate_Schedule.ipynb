{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Mastery_Learning_Rate_Schedule.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMW16+sQlYztQjChki9hQ7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EddyGiusepe/ML_Mastery_Learning_Rate_Schedule/blob/main/ML_Mastery_Learning_Rate_Schedule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F1o5p8nuGNK"
      },
      "source": [
        "# **Tabela de taxas de aprendizagem (Learning Rate Schedule)**\r\n",
        "\r\n",
        "Olá, nesta lição, você descobrirá como configurar uma tabela (cronograma) de taxa de aprendizado adaptável \r\n",
        "para ajustar o modelo durante a execução do treinamento.\r\n",
        "\r\n",
        "A quantidade de mudança no modelo durante cada etapa deste processo de pesquisa, ou o tamanho da etapa, \r\n",
        "é chamada de <font color='orange'>taxa de aprendizado</font> e fornece talvez o hiperparâmetro mais importante a ser ajustado para sua rede neural a fim de obter um bom desempenho em seu problema.\r\n",
        "\r\n",
        "Configurar uma taxa de aprendizagem fixa é muito desafiador e requer experimentação cuidadosa. Uma alternativa ao uso de uma **taxa de aprendizado fixa** é variar a taxa de aprendizado ao longo do processo de treinamento.\r\n",
        "\r\n",
        "\r\n",
        "Keras fornece uma tabela de taxa de aprendizado ``ReduceLROnPlateau`` que ajustará a taxa de aprendizado quando um platô no desempenho do modelo for detectado, <font color='orange'>por exemplo</font>, nenhuma mudança para um determinado número de épocas de treinamento. Por exemplo:\r\n",
        "\r\n",
        "### <font color='blue'>define learning rate schedule</font>\r\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_delta=1E-7, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_GiUWf_wJ6X"
      },
      "source": [
        "Esse retorno de chamada foi projetado para reduzir a taxa de aprendizado depois que o modelo para de melhorar, na esperança de ajustar os pesos do modelo durante o treinamento.\r\n",
        "\r\n",
        "\r\n",
        "O exemplo abaixo demonstra um **Perceptron Multicamadas** com um(a) tabela (cronograma) de taxa de aprendizado em um problema de <font color='orange'>classificação binária</font>, onde a taxa de aprendizado será reduzida em uma ordem de magnitude se nenhuma mudança for detectada na perda de validação em $5$ períodos de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ydhQO2wyq-"
      },
      "source": [
        "### <font color='blue'>Importando a nossas livrarias</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJtSEyChuJi2"
      },
      "source": [
        "from sklearn.datasets import make_circles\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.optimizers import SGD\r\n",
        "from keras.callbacks import ReduceLROnPlateau\r\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK8bwVzVxQMi"
      },
      "source": [
        "### <font color='blue'>Geramos nosso Dataset</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfw2B4MlxMbr"
      },
      "source": [
        "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30WgQcFxxbRw"
      },
      "source": [
        "### <font color='blue'>Dividimos em Dados de Treino e Dados de Teste</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCxS05H2xZ6K"
      },
      "source": [
        "n_train = 500\r\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\r\n",
        "trainy, testy = y[:n_train], y[n_train:]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqahHaJ2xsmP"
      },
      "source": [
        "### <font color='blue'>Construímos nosso modelo</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsY507dxohB"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(50, input_dim=2, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tnk77KyxynS"
      },
      "source": [
        "### <font color='blue'>Compilamos nosso modelo</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd5dG4ogyWtX"
      },
      "source": [
        "opt = SGD(lr=0.01, momentum=0.9)\r\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Z8sNBOyeJ2"
      },
      "source": [
        "### <font color='blue'>Definimos nosso Learning Rate Schedule</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT3UQN5Lyboi"
      },
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_delta=1E-7, verbose=1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtavIQqBy2Ry"
      },
      "source": [
        "### <font color='blue'>Ajustamos nosso modelo (Fit model)</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiqIYD5iyzPu",
        "outputId": "5db145b4-be1b-4f74-e18a-ecff281b9467"
      },
      "source": [
        "history = model.fit(trainX, trainy, validation_data=(testX, testy),epochs=400,verbose=1,callbacks=[rlrp])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 0.7114 - accuracy: 0.4880 - val_loss: 0.7037 - val_accuracy: 0.5020\n",
            "Epoch 2/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7028 - accuracy: 0.4743 - val_loss: 0.7005 - val_accuracy: 0.4840\n",
            "Epoch 3/400\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.7034 - accuracy: 0.4653 - val_loss: 0.6994 - val_accuracy: 0.4540\n",
            "Epoch 4/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7005 - accuracy: 0.4258 - val_loss: 0.6979 - val_accuracy: 0.4700\n",
            "Epoch 5/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.4815 - val_loss: 0.6969 - val_accuracy: 0.4860\n",
            "Epoch 6/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6984 - accuracy: 0.5008 - val_loss: 0.6951 - val_accuracy: 0.4920\n",
            "Epoch 7/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.4966 - val_loss: 0.6939 - val_accuracy: 0.5080\n",
            "Epoch 8/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.6864 - accuracy: 0.5518 - val_loss: 0.6931 - val_accuracy: 0.4880\n",
            "Epoch 9/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6857 - accuracy: 0.5319 - val_loss: 0.6914 - val_accuracy: 0.5040\n",
            "Epoch 10/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.5437 - val_loss: 0.6893 - val_accuracy: 0.5680\n",
            "Epoch 11/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6870 - accuracy: 0.5690 - val_loss: 0.6883 - val_accuracy: 0.5780\n",
            "Epoch 12/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6845 - accuracy: 0.6013 - val_loss: 0.6868 - val_accuracy: 0.5460\n",
            "Epoch 13/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6765 - accuracy: 0.6005 - val_loss: 0.6859 - val_accuracy: 0.5340\n",
            "Epoch 14/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6774 - accuracy: 0.6132 - val_loss: 0.6845 - val_accuracy: 0.5700\n",
            "Epoch 15/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6765 - accuracy: 0.6058 - val_loss: 0.6825 - val_accuracy: 0.5680\n",
            "Epoch 16/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6750 - accuracy: 0.6080 - val_loss: 0.6810 - val_accuracy: 0.5660\n",
            "Epoch 17/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6756 - accuracy: 0.5777 - val_loss: 0.6794 - val_accuracy: 0.5620\n",
            "Epoch 18/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6756 - accuracy: 0.5887 - val_loss: 0.6774 - val_accuracy: 0.5840\n",
            "Epoch 19/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6699 - accuracy: 0.6430 - val_loss: 0.6765 - val_accuracy: 0.5760\n",
            "Epoch 20/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6653 - accuracy: 0.6476 - val_loss: 0.6749 - val_accuracy: 0.5800\n",
            "Epoch 21/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6679 - accuracy: 0.6382 - val_loss: 0.6730 - val_accuracy: 0.5920\n",
            "Epoch 22/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6687 - accuracy: 0.6457 - val_loss: 0.6715 - val_accuracy: 0.6060\n",
            "Epoch 23/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6642 - accuracy: 0.6661 - val_loss: 0.6694 - val_accuracy: 0.6180\n",
            "Epoch 24/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6608 - accuracy: 0.6801 - val_loss: 0.6679 - val_accuracy: 0.6160\n",
            "Epoch 25/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6571 - accuracy: 0.6913 - val_loss: 0.6661 - val_accuracy: 0.6200\n",
            "Epoch 26/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6547 - accuracy: 0.6918 - val_loss: 0.6642 - val_accuracy: 0.6320\n",
            "Epoch 27/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6578 - accuracy: 0.6701 - val_loss: 0.6623 - val_accuracy: 0.6340\n",
            "Epoch 28/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6572 - accuracy: 0.6878 - val_loss: 0.6608 - val_accuracy: 0.6520\n",
            "Epoch 29/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6556 - accuracy: 0.6579 - val_loss: 0.6593 - val_accuracy: 0.6460\n",
            "Epoch 30/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6578 - accuracy: 0.6714 - val_loss: 0.6575 - val_accuracy: 0.6520\n",
            "Epoch 31/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6441 - accuracy: 0.7118 - val_loss: 0.6568 - val_accuracy: 0.6400\n",
            "Epoch 32/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6469 - accuracy: 0.6853 - val_loss: 0.6540 - val_accuracy: 0.6800\n",
            "Epoch 33/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6451 - accuracy: 0.7150 - val_loss: 0.6518 - val_accuracy: 0.6900\n",
            "Epoch 34/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6457 - accuracy: 0.7246 - val_loss: 0.6495 - val_accuracy: 0.7080\n",
            "Epoch 35/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6459 - accuracy: 0.7127 - val_loss: 0.6483 - val_accuracy: 0.6980\n",
            "Epoch 36/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6457 - accuracy: 0.6776 - val_loss: 0.6463 - val_accuracy: 0.6920\n",
            "Epoch 37/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6395 - accuracy: 0.7164 - val_loss: 0.6436 - val_accuracy: 0.7260\n",
            "Epoch 38/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6317 - accuracy: 0.7467 - val_loss: 0.6422 - val_accuracy: 0.7140\n",
            "Epoch 39/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6349 - accuracy: 0.7204 - val_loss: 0.6398 - val_accuracy: 0.7340\n",
            "Epoch 40/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6341 - accuracy: 0.7429 - val_loss: 0.6375 - val_accuracy: 0.7440\n",
            "Epoch 41/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6280 - accuracy: 0.7692 - val_loss: 0.6364 - val_accuracy: 0.7260\n",
            "Epoch 42/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6256 - accuracy: 0.7649 - val_loss: 0.6340 - val_accuracy: 0.7400\n",
            "Epoch 43/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6296 - accuracy: 0.7170 - val_loss: 0.6320 - val_accuracy: 0.7280\n",
            "Epoch 44/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6223 - accuracy: 0.7663 - val_loss: 0.6296 - val_accuracy: 0.7420\n",
            "Epoch 45/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.6184 - accuracy: 0.7672 - val_loss: 0.6274 - val_accuracy: 0.7480\n",
            "Epoch 46/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6096 - accuracy: 0.7964 - val_loss: 0.6252 - val_accuracy: 0.7540\n",
            "Epoch 47/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.6087 - accuracy: 0.7781 - val_loss: 0.6223 - val_accuracy: 0.7620\n",
            "Epoch 48/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6165 - accuracy: 0.7627 - val_loss: 0.6195 - val_accuracy: 0.7660\n",
            "Epoch 49/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6077 - accuracy: 0.8073 - val_loss: 0.6180 - val_accuracy: 0.7700\n",
            "Epoch 50/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5993 - accuracy: 0.8000 - val_loss: 0.6148 - val_accuracy: 0.7880\n",
            "Epoch 51/400\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6044 - accuracy: 0.7921 - val_loss: 0.6133 - val_accuracy: 0.7720\n",
            "Epoch 52/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5981 - accuracy: 0.8084 - val_loss: 0.6108 - val_accuracy: 0.7840\n",
            "Epoch 53/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6039 - accuracy: 0.7765 - val_loss: 0.6079 - val_accuracy: 0.7960\n",
            "Epoch 54/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5965 - accuracy: 0.8011 - val_loss: 0.6061 - val_accuracy: 0.7880\n",
            "Epoch 55/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6102 - accuracy: 0.7888 - val_loss: 0.6030 - val_accuracy: 0.7960\n",
            "Epoch 56/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5853 - accuracy: 0.8175 - val_loss: 0.6023 - val_accuracy: 0.7920\n",
            "Epoch 57/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5860 - accuracy: 0.8090 - val_loss: 0.5987 - val_accuracy: 0.7880\n",
            "Epoch 58/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5896 - accuracy: 0.8100 - val_loss: 0.5956 - val_accuracy: 0.8040\n",
            "Epoch 59/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5768 - accuracy: 0.8340 - val_loss: 0.5937 - val_accuracy: 0.8000\n",
            "Epoch 60/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5887 - accuracy: 0.7800 - val_loss: 0.5904 - val_accuracy: 0.8240\n",
            "Epoch 61/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5876 - accuracy: 0.8097 - val_loss: 0.5890 - val_accuracy: 0.8020\n",
            "Epoch 62/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5794 - accuracy: 0.7956 - val_loss: 0.5864 - val_accuracy: 0.7980\n",
            "Epoch 63/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5745 - accuracy: 0.8015 - val_loss: 0.5832 - val_accuracy: 0.8200\n",
            "Epoch 64/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5649 - accuracy: 0.8325 - val_loss: 0.5814 - val_accuracy: 0.8040\n",
            "Epoch 65/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5780 - accuracy: 0.8206 - val_loss: 0.5770 - val_accuracy: 0.8080\n",
            "Epoch 66/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5667 - accuracy: 0.8185 - val_loss: 0.5750 - val_accuracy: 0.8260\n",
            "Epoch 67/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5634 - accuracy: 0.8224 - val_loss: 0.5729 - val_accuracy: 0.8100\n",
            "Epoch 68/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5612 - accuracy: 0.8305 - val_loss: 0.5699 - val_accuracy: 0.8040\n",
            "Epoch 69/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5562 - accuracy: 0.8396 - val_loss: 0.5685 - val_accuracy: 0.8220\n",
            "Epoch 70/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5646 - accuracy: 0.8192 - val_loss: 0.5640 - val_accuracy: 0.8240\n",
            "Epoch 71/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5574 - accuracy: 0.8077 - val_loss: 0.5613 - val_accuracy: 0.8240\n",
            "Epoch 72/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5510 - accuracy: 0.8442 - val_loss: 0.5591 - val_accuracy: 0.8200\n",
            "Epoch 73/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5586 - accuracy: 0.8027 - val_loss: 0.5570 - val_accuracy: 0.8220\n",
            "Epoch 74/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5294 - accuracy: 0.8504 - val_loss: 0.5544 - val_accuracy: 0.8200\n",
            "Epoch 75/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5385 - accuracy: 0.8459 - val_loss: 0.5505 - val_accuracy: 0.8200\n",
            "Epoch 76/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5422 - accuracy: 0.8393 - val_loss: 0.5470 - val_accuracy: 0.8280\n",
            "Epoch 77/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5455 - accuracy: 0.8253 - val_loss: 0.5444 - val_accuracy: 0.8320\n",
            "Epoch 78/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5393 - accuracy: 0.8339 - val_loss: 0.5435 - val_accuracy: 0.8240\n",
            "Epoch 79/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5280 - accuracy: 0.8391 - val_loss: 0.5408 - val_accuracy: 0.8240\n",
            "Epoch 80/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5248 - accuracy: 0.8144 - val_loss: 0.5364 - val_accuracy: 0.8300\n",
            "Epoch 81/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5294 - accuracy: 0.8458 - val_loss: 0.5342 - val_accuracy: 0.8300\n",
            "Epoch 82/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5207 - accuracy: 0.8590 - val_loss: 0.5318 - val_accuracy: 0.8200\n",
            "Epoch 83/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5273 - accuracy: 0.8371 - val_loss: 0.5267 - val_accuracy: 0.8360\n",
            "Epoch 84/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4992 - accuracy: 0.8588 - val_loss: 0.5262 - val_accuracy: 0.8300\n",
            "Epoch 85/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5055 - accuracy: 0.8585 - val_loss: 0.5225 - val_accuracy: 0.8400\n",
            "Epoch 86/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5285 - accuracy: 0.8070 - val_loss: 0.5215 - val_accuracy: 0.8280\n",
            "Epoch 87/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5101 - accuracy: 0.8515 - val_loss: 0.5159 - val_accuracy: 0.8480\n",
            "Epoch 88/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5018 - accuracy: 0.8512 - val_loss: 0.5146 - val_accuracy: 0.8320\n",
            "Epoch 89/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5102 - accuracy: 0.8197 - val_loss: 0.5127 - val_accuracy: 0.8320\n",
            "Epoch 90/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4966 - accuracy: 0.8467 - val_loss: 0.5105 - val_accuracy: 0.8420\n",
            "Epoch 91/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5051 - accuracy: 0.8337 - val_loss: 0.5069 - val_accuracy: 0.8420\n",
            "Epoch 92/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4904 - accuracy: 0.8278 - val_loss: 0.5041 - val_accuracy: 0.8440\n",
            "Epoch 93/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5021 - accuracy: 0.8367 - val_loss: 0.5023 - val_accuracy: 0.8400\n",
            "Epoch 94/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4903 - accuracy: 0.8483 - val_loss: 0.4990 - val_accuracy: 0.8380\n",
            "Epoch 95/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5036 - accuracy: 0.8222 - val_loss: 0.4984 - val_accuracy: 0.8480\n",
            "Epoch 96/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4978 - accuracy: 0.8436 - val_loss: 0.4944 - val_accuracy: 0.8500\n",
            "Epoch 97/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5014 - accuracy: 0.8334 - val_loss: 0.4938 - val_accuracy: 0.8340\n",
            "Epoch 98/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4738 - accuracy: 0.8480 - val_loss: 0.4881 - val_accuracy: 0.8480\n",
            "Epoch 99/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4836 - accuracy: 0.8500 - val_loss: 0.4868 - val_accuracy: 0.8400\n",
            "Epoch 100/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4777 - accuracy: 0.8342 - val_loss: 0.4847 - val_accuracy: 0.8460\n",
            "Epoch 101/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4778 - accuracy: 0.8455 - val_loss: 0.4816 - val_accuracy: 0.8600\n",
            "Epoch 102/400\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.4836 - accuracy: 0.8576 - val_loss: 0.4820 - val_accuracy: 0.8380\n",
            "Epoch 103/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4875 - accuracy: 0.8170 - val_loss: 0.4761 - val_accuracy: 0.8400\n",
            "Epoch 104/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4721 - accuracy: 0.8555 - val_loss: 0.4766 - val_accuracy: 0.8460\n",
            "Epoch 105/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4716 - accuracy: 0.8521 - val_loss: 0.4759 - val_accuracy: 0.8540\n",
            "Epoch 106/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4789 - accuracy: 0.8292 - val_loss: 0.4703 - val_accuracy: 0.8420\n",
            "Epoch 107/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4628 - accuracy: 0.8432 - val_loss: 0.4685 - val_accuracy: 0.8400\n",
            "Epoch 108/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4788 - accuracy: 0.8184 - val_loss: 0.4666 - val_accuracy: 0.8620\n",
            "Epoch 109/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4549 - accuracy: 0.8587 - val_loss: 0.4649 - val_accuracy: 0.8620\n",
            "Epoch 110/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4542 - accuracy: 0.8512 - val_loss: 0.4638 - val_accuracy: 0.8420\n",
            "Epoch 111/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4338 - accuracy: 0.8622 - val_loss: 0.4585 - val_accuracy: 0.8420\n",
            "Epoch 112/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4454 - accuracy: 0.8416 - val_loss: 0.4599 - val_accuracy: 0.8580\n",
            "Epoch 113/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4522 - accuracy: 0.8416 - val_loss: 0.4572 - val_accuracy: 0.8580\n",
            "Epoch 114/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4609 - accuracy: 0.8267 - val_loss: 0.4558 - val_accuracy: 0.8480\n",
            "Epoch 115/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4707 - accuracy: 0.8397 - val_loss: 0.4511 - val_accuracy: 0.8540\n",
            "Epoch 116/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4627 - accuracy: 0.8327 - val_loss: 0.4526 - val_accuracy: 0.8560\n",
            "Epoch 117/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4549 - accuracy: 0.8313 - val_loss: 0.4500 - val_accuracy: 0.8520\n",
            "Epoch 118/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4693 - accuracy: 0.8082 - val_loss: 0.4464 - val_accuracy: 0.8580\n",
            "Epoch 119/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4454 - accuracy: 0.8393 - val_loss: 0.4450 - val_accuracy: 0.8640\n",
            "Epoch 120/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4332 - accuracy: 0.8631 - val_loss: 0.4424 - val_accuracy: 0.8480\n",
            "Epoch 121/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4518 - accuracy: 0.8390 - val_loss: 0.4434 - val_accuracy: 0.8540\n",
            "Epoch 122/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4500 - accuracy: 0.8443 - val_loss: 0.4411 - val_accuracy: 0.8540\n",
            "Epoch 123/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4506 - accuracy: 0.8253 - val_loss: 0.4377 - val_accuracy: 0.8560\n",
            "Epoch 124/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4388 - accuracy: 0.8458 - val_loss: 0.4358 - val_accuracy: 0.8600\n",
            "Epoch 125/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4356 - accuracy: 0.8473 - val_loss: 0.4366 - val_accuracy: 0.8380\n",
            "Epoch 126/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4262 - accuracy: 0.8506 - val_loss: 0.4357 - val_accuracy: 0.8460\n",
            "Epoch 127/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4502 - accuracy: 0.8495 - val_loss: 0.4322 - val_accuracy: 0.8660\n",
            "Epoch 128/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4377 - accuracy: 0.8332 - val_loss: 0.4317 - val_accuracy: 0.8560\n",
            "Epoch 129/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4391 - accuracy: 0.8203 - val_loss: 0.4281 - val_accuracy: 0.8500\n",
            "Epoch 130/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4288 - accuracy: 0.8371 - val_loss: 0.4294 - val_accuracy: 0.8560\n",
            "Epoch 131/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4287 - accuracy: 0.8453 - val_loss: 0.4246 - val_accuracy: 0.8600\n",
            "Epoch 132/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4196 - accuracy: 0.8425 - val_loss: 0.4257 - val_accuracy: 0.8500\n",
            "Epoch 133/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4404 - accuracy: 0.8248 - val_loss: 0.4217 - val_accuracy: 0.8560\n",
            "Epoch 134/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4300 - accuracy: 0.8341 - val_loss: 0.4222 - val_accuracy: 0.8600\n",
            "Epoch 135/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4334 - accuracy: 0.8246 - val_loss: 0.4236 - val_accuracy: 0.8560\n",
            "Epoch 136/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4330 - accuracy: 0.8352 - val_loss: 0.4182 - val_accuracy: 0.8400\n",
            "Epoch 137/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4241 - accuracy: 0.8386 - val_loss: 0.4176 - val_accuracy: 0.8600\n",
            "Epoch 138/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4253 - accuracy: 0.8450 - val_loss: 0.4176 - val_accuracy: 0.8620\n",
            "Epoch 139/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4134 - accuracy: 0.8459 - val_loss: 0.4139 - val_accuracy: 0.8580\n",
            "Epoch 140/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4539 - accuracy: 0.8098 - val_loss: 0.4138 - val_accuracy: 0.8460\n",
            "Epoch 141/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8191 - val_loss: 0.4151 - val_accuracy: 0.8600\n",
            "Epoch 142/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4292 - accuracy: 0.8380 - val_loss: 0.4104 - val_accuracy: 0.8480\n",
            "Epoch 143/400\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.4381 - accuracy: 0.8187 - val_loss: 0.4127 - val_accuracy: 0.8480\n",
            "Epoch 144/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4121 - accuracy: 0.8366 - val_loss: 0.4104 - val_accuracy: 0.8620\n",
            "Epoch 145/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3992 - accuracy: 0.8523 - val_loss: 0.4077 - val_accuracy: 0.8540\n",
            "Epoch 146/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4322 - accuracy: 0.8372 - val_loss: 0.4061 - val_accuracy: 0.8520\n",
            "Epoch 147/400\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4171 - accuracy: 0.8346 - val_loss: 0.4073 - val_accuracy: 0.8500\n",
            "Epoch 148/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3972 - accuracy: 0.8595 - val_loss: 0.4055 - val_accuracy: 0.8600\n",
            "Epoch 149/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.8254 - val_loss: 0.4029 - val_accuracy: 0.8460\n",
            "Epoch 150/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4122 - accuracy: 0.8194 - val_loss: 0.4025 - val_accuracy: 0.8500\n",
            "Epoch 151/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4056 - accuracy: 0.8269 - val_loss: 0.4016 - val_accuracy: 0.8500\n",
            "Epoch 152/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4241 - accuracy: 0.8139 - val_loss: 0.4015 - val_accuracy: 0.8520\n",
            "Epoch 153/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4163 - accuracy: 0.8292 - val_loss: 0.4008 - val_accuracy: 0.8500\n",
            "Epoch 154/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8359 - val_loss: 0.4009 - val_accuracy: 0.8540\n",
            "Epoch 155/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.8250 - val_loss: 0.3995 - val_accuracy: 0.8520\n",
            "Epoch 156/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4362 - accuracy: 0.7958 - val_loss: 0.3974 - val_accuracy: 0.8500\n",
            "Epoch 157/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4178 - accuracy: 0.8272 - val_loss: 0.4011 - val_accuracy: 0.8460\n",
            "Epoch 158/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4382 - accuracy: 0.8166 - val_loss: 0.3952 - val_accuracy: 0.8500\n",
            "Epoch 159/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4131 - accuracy: 0.8168 - val_loss: 0.3949 - val_accuracy: 0.8540\n",
            "Epoch 160/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4031 - accuracy: 0.8456 - val_loss: 0.3929 - val_accuracy: 0.8500\n",
            "Epoch 161/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4167 - accuracy: 0.8096 - val_loss: 0.3937 - val_accuracy: 0.8500\n",
            "Epoch 162/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3816 - accuracy: 0.8675 - val_loss: 0.3937 - val_accuracy: 0.8640\n",
            "Epoch 163/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4047 - accuracy: 0.8246 - val_loss: 0.3920 - val_accuracy: 0.8520\n",
            "Epoch 164/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.8559 - val_loss: 0.3927 - val_accuracy: 0.8520\n",
            "Epoch 165/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4068 - accuracy: 0.8453 - val_loss: 0.3885 - val_accuracy: 0.8480\n",
            "Epoch 166/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4109 - accuracy: 0.8317 - val_loss: 0.3897 - val_accuracy: 0.8540\n",
            "Epoch 167/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4252 - accuracy: 0.8227 - val_loss: 0.3889 - val_accuracy: 0.8540\n",
            "Epoch 168/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4177 - accuracy: 0.8252 - val_loss: 0.3887 - val_accuracy: 0.8520\n",
            "Epoch 169/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4180 - accuracy: 0.8317 - val_loss: 0.3894 - val_accuracy: 0.8500\n",
            "Epoch 170/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4073 - accuracy: 0.8278 - val_loss: 0.3868 - val_accuracy: 0.8560\n",
            "Epoch 171/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3949 - accuracy: 0.8306 - val_loss: 0.3873 - val_accuracy: 0.8540\n",
            "Epoch 172/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4095 - accuracy: 0.8328 - val_loss: 0.3853 - val_accuracy: 0.8520\n",
            "Epoch 173/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3984 - accuracy: 0.8412 - val_loss: 0.3846 - val_accuracy: 0.8520\n",
            "Epoch 174/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3949 - accuracy: 0.8395 - val_loss: 0.3890 - val_accuracy: 0.8460\n",
            "Epoch 175/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3794 - accuracy: 0.8623 - val_loss: 0.3831 - val_accuracy: 0.8540\n",
            "Epoch 176/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3790 - accuracy: 0.8451 - val_loss: 0.3817 - val_accuracy: 0.8500\n",
            "Epoch 177/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4092 - accuracy: 0.8224 - val_loss: 0.3865 - val_accuracy: 0.8480\n",
            "Epoch 178/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3997 - accuracy: 0.8314 - val_loss: 0.3834 - val_accuracy: 0.8460\n",
            "Epoch 179/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3911 - accuracy: 0.8350 - val_loss: 0.3838 - val_accuracy: 0.8600\n",
            "Epoch 180/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4173 - accuracy: 0.8324 - val_loss: 0.3811 - val_accuracy: 0.8520\n",
            "Epoch 181/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4036 - accuracy: 0.8226 - val_loss: 0.3818 - val_accuracy: 0.8520\n",
            "Epoch 182/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3928 - accuracy: 0.8326 - val_loss: 0.3812 - val_accuracy: 0.8540\n",
            "Epoch 183/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3969 - accuracy: 0.8338 - val_loss: 0.3806 - val_accuracy: 0.8500\n",
            "Epoch 184/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3917 - accuracy: 0.8426 - val_loss: 0.3790 - val_accuracy: 0.8520\n",
            "Epoch 185/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4029 - accuracy: 0.8243 - val_loss: 0.3822 - val_accuracy: 0.8500\n",
            "Epoch 186/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4069 - accuracy: 0.8302 - val_loss: 0.3789 - val_accuracy: 0.8540\n",
            "Epoch 187/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4015 - accuracy: 0.8119 - val_loss: 0.3774 - val_accuracy: 0.8500\n",
            "Epoch 188/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3881 - accuracy: 0.8272 - val_loss: 0.3792 - val_accuracy: 0.8600\n",
            "Epoch 189/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3992 - accuracy: 0.8304 - val_loss: 0.3759 - val_accuracy: 0.8520\n",
            "Epoch 190/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3852 - accuracy: 0.8445 - val_loss: 0.3810 - val_accuracy: 0.8560\n",
            "Epoch 191/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3859 - accuracy: 0.8432 - val_loss: 0.3767 - val_accuracy: 0.8520\n",
            "Epoch 192/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3754 - accuracy: 0.8450 - val_loss: 0.3743 - val_accuracy: 0.8500\n",
            "Epoch 193/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3891 - accuracy: 0.8282 - val_loss: 0.3787 - val_accuracy: 0.8520\n",
            "Epoch 194/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4097 - accuracy: 0.8211 - val_loss: 0.3749 - val_accuracy: 0.8520\n",
            "Epoch 195/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3691 - accuracy: 0.8466 - val_loss: 0.3763 - val_accuracy: 0.8500\n",
            "Epoch 196/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3581 - accuracy: 0.8621 - val_loss: 0.3754 - val_accuracy: 0.8500\n",
            "Epoch 197/400\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.4106 - accuracy: 0.8109 - val_loss: 0.3764 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00197: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 198/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3607 - accuracy: 0.8628 - val_loss: 0.3796 - val_accuracy: 0.8540\n",
            "Epoch 199/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3986 - accuracy: 0.8262 - val_loss: 0.3793 - val_accuracy: 0.8560\n",
            "Epoch 200/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3689 - accuracy: 0.8405 - val_loss: 0.3777 - val_accuracy: 0.8580\n",
            "Epoch 201/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3808 - accuracy: 0.8466 - val_loss: 0.3764 - val_accuracy: 0.8540\n",
            "Epoch 202/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3602 - accuracy: 0.8572 - val_loss: 0.3756 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00202: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "Epoch 203/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.8561 - val_loss: 0.3749 - val_accuracy: 0.8540\n",
            "Epoch 204/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3819 - accuracy: 0.8361 - val_loss: 0.3748 - val_accuracy: 0.8560\n",
            "Epoch 205/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3726 - accuracy: 0.8427 - val_loss: 0.3747 - val_accuracy: 0.8540\n",
            "Epoch 206/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4017 - accuracy: 0.8302 - val_loss: 0.3747 - val_accuracy: 0.8540\n",
            "Epoch 207/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3598 - accuracy: 0.8610 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00207: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
            "Epoch 208/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3719 - accuracy: 0.8546 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 209/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3953 - accuracy: 0.8396 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 210/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4064 - accuracy: 0.8419 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 211/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4049 - accuracy: 0.8322 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 212/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3653 - accuracy: 0.8447 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00212: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
            "Epoch 213/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3931 - accuracy: 0.8367 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 214/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8403 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 215/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3901 - accuracy: 0.8323 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 216/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3756 - accuracy: 0.8409 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 217/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4000 - accuracy: 0.8231 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00217: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
            "Epoch 218/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4017 - accuracy: 0.8264 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 219/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3705 - accuracy: 0.8525 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 220/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3736 - accuracy: 0.8284 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 221/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3920 - accuracy: 0.8362 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 222/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3822 - accuracy: 0.8365 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00222: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
            "Epoch 223/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3822 - accuracy: 0.8388 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 224/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3665 - accuracy: 0.8586 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 225/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3671 - accuracy: 0.8445 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 226/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3964 - accuracy: 0.8299 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 227/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3853 - accuracy: 0.8405 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00227: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
            "Epoch 228/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3755 - accuracy: 0.8292 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 229/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3887 - accuracy: 0.8398 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 230/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3874 - accuracy: 0.8368 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 231/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4057 - accuracy: 0.8224 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 232/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3978 - accuracy: 0.8236 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00232: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
            "Epoch 233/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3881 - accuracy: 0.8260 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 234/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3826 - accuracy: 0.8343 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 235/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3896 - accuracy: 0.8454 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 236/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.8366 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 237/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3668 - accuracy: 0.8359 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00237: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
            "Epoch 238/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3709 - accuracy: 0.8540 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 239/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8354 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 240/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3810 - accuracy: 0.8325 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 241/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3785 - accuracy: 0.8426 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 242/400\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.3705 - accuracy: 0.8356 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00242: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
            "Epoch 243/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3869 - accuracy: 0.8400 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 244/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4128 - accuracy: 0.8265 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 245/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.8398 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 246/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3852 - accuracy: 0.8356 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 247/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3764 - accuracy: 0.8400 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00247: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
            "Epoch 248/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4126 - accuracy: 0.8127 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 249/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3773 - accuracy: 0.8438 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 250/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4037 - accuracy: 0.8429 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 251/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4052 - accuracy: 0.8459 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 252/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3463 - accuracy: 0.8508 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00252: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
            "Epoch 253/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4088 - accuracy: 0.8199 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 254/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3532 - accuracy: 0.8635 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 255/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3676 - accuracy: 0.8601 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 256/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3674 - accuracy: 0.8419 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 257/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3895 - accuracy: 0.8281 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00257: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
            "Epoch 258/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3874 - accuracy: 0.8288 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 259/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3867 - accuracy: 0.8351 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 260/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.8476 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 261/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3674 - accuracy: 0.8520 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 262/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3444 - accuracy: 0.8660 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00262: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
            "Epoch 263/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4022 - accuracy: 0.8131 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 264/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3768 - accuracy: 0.8449 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 265/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4179 - accuracy: 0.8189 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 266/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4033 - accuracy: 0.8259 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 267/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3750 - accuracy: 0.8372 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00267: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
            "Epoch 268/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3823 - accuracy: 0.8375 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 269/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3925 - accuracy: 0.8333 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 270/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3950 - accuracy: 0.8281 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 271/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.8337 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 272/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4173 - accuracy: 0.7992 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00272: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
            "Epoch 273/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3921 - accuracy: 0.8271 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 274/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4005 - accuracy: 0.8259 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 275/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3977 - accuracy: 0.8405 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 276/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4028 - accuracy: 0.8296 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 277/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3662 - accuracy: 0.8614 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00277: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
            "Epoch 278/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3804 - accuracy: 0.8326 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 279/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3651 - accuracy: 0.8526 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 280/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4077 - accuracy: 0.8213 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 281/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3611 - accuracy: 0.8451 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 282/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3773 - accuracy: 0.8327 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00282: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
            "Epoch 283/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3808 - accuracy: 0.8461 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 284/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3910 - accuracy: 0.8264 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 285/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3841 - accuracy: 0.8443 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 286/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3611 - accuracy: 0.8493 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 287/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3963 - accuracy: 0.8235 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00287: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
            "Epoch 288/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3664 - accuracy: 0.8493 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 289/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3974 - accuracy: 0.8424 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 290/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3981 - accuracy: 0.8290 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 291/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4092 - accuracy: 0.8274 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 292/400\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.3860 - accuracy: 0.8302 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00292: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
            "Epoch 293/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3901 - accuracy: 0.8289 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 294/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3989 - accuracy: 0.8277 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 295/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3852 - accuracy: 0.8349 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 296/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3744 - accuracy: 0.8395 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 297/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3759 - accuracy: 0.8504 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00297: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
            "Epoch 298/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3715 - accuracy: 0.8285 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 299/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3928 - accuracy: 0.8327 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 300/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4062 - accuracy: 0.8168 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 301/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4113 - accuracy: 0.8197 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 302/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3773 - accuracy: 0.8408 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00302: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
            "Epoch 303/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3925 - accuracy: 0.8336 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 304/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3627 - accuracy: 0.8537 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 305/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3799 - accuracy: 0.8449 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 306/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.8571 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 307/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3925 - accuracy: 0.8315 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00307: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
            "Epoch 308/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4018 - accuracy: 0.8314 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 309/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3849 - accuracy: 0.8407 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 310/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3837 - accuracy: 0.8432 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 311/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3700 - accuracy: 0.8317 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 312/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3907 - accuracy: 0.8249 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00312: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
            "Epoch 313/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4084 - accuracy: 0.8142 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 314/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3799 - accuracy: 0.8372 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 315/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3888 - accuracy: 0.8271 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 316/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3714 - accuracy: 0.8381 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 317/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3913 - accuracy: 0.8368 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00317: ReduceLROnPlateau reducing learning rate to 9.999999887266024e-28.\n",
            "Epoch 318/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4079 - accuracy: 0.8248 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 319/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3646 - accuracy: 0.8451 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 320/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3835 - accuracy: 0.8353 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 321/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3665 - accuracy: 0.8405 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 322/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4026 - accuracy: 0.8227 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00322: ReduceLROnPlateau reducing learning rate to 1.0000000272452012e-28.\n",
            "Epoch 323/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3649 - accuracy: 0.8404 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 324/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3885 - accuracy: 0.8272 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 325/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3966 - accuracy: 0.8278 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 326/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3932 - accuracy: 0.8381 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 327/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3788 - accuracy: 0.8348 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00327: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-29.\n",
            "Epoch 328/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3596 - accuracy: 0.8523 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 329/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3808 - accuracy: 0.8368 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 330/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3792 - accuracy: 0.8510 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 331/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4293 - accuracy: 0.7977 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 332/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3771 - accuracy: 0.8361 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00332: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-30.\n",
            "Epoch 333/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3827 - accuracy: 0.8280 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 334/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4028 - accuracy: 0.8157 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 335/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3687 - accuracy: 0.8454 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 336/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3980 - accuracy: 0.8201 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 337/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4031 - accuracy: 0.8230 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00337: ReduceLROnPlateau reducing learning rate to 1.000000003171077e-31.\n",
            "Epoch 338/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4020 - accuracy: 0.8212 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 339/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3849 - accuracy: 0.8454 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 340/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3709 - accuracy: 0.8581 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 341/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3798 - accuracy: 0.8362 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 342/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4029 - accuracy: 0.8353 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00342: ReduceLROnPlateau reducing learning rate to 9.999999796611899e-33.\n",
            "Epoch 343/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3779 - accuracy: 0.8380 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 344/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3738 - accuracy: 0.8492 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 345/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4181 - accuracy: 0.8257 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 346/400\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4040 - accuracy: 0.8014 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 347/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3934 - accuracy: 0.8384 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00347: ReduceLROnPlateau reducing learning rate to 9.999999502738312e-34.\n",
            "Epoch 348/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3798 - accuracy: 0.8330 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 349/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3948 - accuracy: 0.8239 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 350/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3874 - accuracy: 0.8279 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 351/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3840 - accuracy: 0.8280 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 352/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3811 - accuracy: 0.8474 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00352: ReduceLROnPlateau reducing learning rate to 9.999999319067318e-35.\n",
            "Epoch 353/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3987 - accuracy: 0.8400 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 354/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3817 - accuracy: 0.8483 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 355/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3874 - accuracy: 0.8381 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 356/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3814 - accuracy: 0.8384 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 357/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3910 - accuracy: 0.8326 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00357: ReduceLROnPlateau reducing learning rate to 9.999999319067319e-36.\n",
            "Epoch 358/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3946 - accuracy: 0.8259 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 359/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3820 - accuracy: 0.8263 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 360/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3601 - accuracy: 0.8625 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 361/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3897 - accuracy: 0.8275 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 362/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3981 - accuracy: 0.8076 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00362: ReduceLROnPlateau reducing learning rate to 9.999999462560281e-37.\n",
            "Epoch 363/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3968 - accuracy: 0.8372 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 364/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3788 - accuracy: 0.8360 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 365/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4143 - accuracy: 0.8234 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 366/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3942 - accuracy: 0.8352 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 367/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3788 - accuracy: 0.8417 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00367: ReduceLROnPlateau reducing learning rate to 9.99999946256028e-38.\n",
            "Epoch 368/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3964 - accuracy: 0.8226 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 369/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3872 - accuracy: 0.8393 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 370/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3887 - accuracy: 0.8259 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 371/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3927 - accuracy: 0.8254 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 372/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4074 - accuracy: 0.8231 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00372: ReduceLROnPlateau reducing learning rate to 9.99999991097579e-39.\n",
            "Epoch 373/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3905 - accuracy: 0.8412 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 374/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4017 - accuracy: 0.8200 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 375/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3805 - accuracy: 0.8325 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 376/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3839 - accuracy: 0.8517 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 377/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8161 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00377: ReduceLROnPlateau reducing learning rate to 9.999999350456405e-40.\n",
            "Epoch 378/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3722 - accuracy: 0.8315 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 379/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3655 - accuracy: 0.8478 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 380/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.8334 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 381/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3772 - accuracy: 0.8366 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 382/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4100 - accuracy: 0.8137 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00382: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n",
            "Epoch 383/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3872 - accuracy: 0.8352 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 384/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3773 - accuracy: 0.8379 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 385/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3944 - accuracy: 0.8182 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 386/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3919 - accuracy: 0.8293 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 387/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3647 - accuracy: 0.8470 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00387: ReduceLROnPlateau reducing learning rate to 9.99994610111476e-42.\n",
            "Epoch 388/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3626 - accuracy: 0.8557 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 389/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3769 - accuracy: 0.8432 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 390/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3759 - accuracy: 0.8513 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 391/400\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.3668 - accuracy: 0.8474 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 392/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3700 - accuracy: 0.8435 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00392: ReduceLROnPlateau reducing learning rate to 9.999665841421895e-43.\n",
            "Epoch 393/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3779 - accuracy: 0.8360 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 394/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3773 - accuracy: 0.8354 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 395/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3765 - accuracy: 0.8450 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 396/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3713 - accuracy: 0.8522 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 397/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3765 - accuracy: 0.8483 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00397: ReduceLROnPlateau reducing learning rate to 1.0005271035279195e-43.\n",
            "Epoch 398/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4125 - accuracy: 0.8041 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 399/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3993 - accuracy: 0.8196 - val_loss: 0.3746 - val_accuracy: 0.8540\n",
            "Epoch 400/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3668 - accuracy: 0.8441 - val_loss: 0.3746 - val_accuracy: 0.8540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGM78anDy3no"
      },
      "source": [
        "### <font color='blue'>Avaliamos nosso modelo</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXp5WBjfzUpK",
        "outputId": "97414d74-74ca-49a8-ae47-66a5ea2a85b9"
      },
      "source": [
        "_, train_acc = model.evaluate(trainX, trainy, verbose=1)\r\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3842 - accuracy: 0.8360\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3746 - accuracy: 0.8540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoYoUimvzep3",
        "outputId": "dab289f9-e5a6-46c8-e60b-3c935e8fa203"
      },
      "source": [
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.836, Test: 0.854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf79Btg-zoYK"
      },
      "source": [
        "### <font color='blue'>Plot loss learning curves</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "NW9GfRdJ0BP-",
        "outputId": "aa48316d-5d39-4995-d7f5-8e5b99d58cf6"
      },
      "source": [
        "# Gráfico das curvas de aprendizagem da função Loss\r\n",
        "\r\n",
        "#pyplot.subplot(211)\r\n",
        "pyplot.title('Cross-Entropy Loss', pad=-40)\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkKqnxLG0V_d"
      },
      "source": [
        "### <font color='blue'>Plot accuracy learning curves</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "NkBLpP830EPa",
        "outputId": "f7db035f-b019-490b-8db9-f10c4b350bb2"
      },
      "source": [
        "# Gráfico das curvas de aprendizagem da precisão (acurácia)\r\n",
        "\r\n",
        "#pyplot.subplot(212)\r\n",
        "pyplot.title('Accuracy', pad=-40)\r\n",
        "pyplot.plot(history.history['accuracy'], label='train')\r\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEFCAYAAADzHRw3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dcnk33fgSRAwiYgKiAiiIpiEbAqalu32uptq+2t223Vq7Zu9drW2/5cW9d6XVqr1mrdUVEEQVF2qOwJexLISvZ1Zr6/P85JZhImECCZYWY+z8cjj5nzPefMfHII7/nO92xijEEppVTwiwh0AUoppfqGBrpSSoUIDXSllAoRGuhKKRUiNNCVUipEaKArpVSI0EBXSqkQoYGugo6ILBKR/SISE+halDqWaKCroCIi+cAZgAEu9OP7RvrrvZQ6UhroKtj8EPgaeBG4uqNRRAaLyL9EpEJEqkTkz17zrhWRTSJSLyIbRWSi3W5EZITXci+KyAP287NEpFhEbheRfcALIpImIu/b77Hffp7ntX66iLwgIqX2/Lft9vUicoHXclEiUikiE/ptK6mwpIGugs0Pgb/bP7NEZICIOID3gV1APpALvAYgIt8D7rPXS8bq1Vf18r0GAunAUOA6rP8vL9jTQ4Bm4M9ey/8NiAeOB7KBR+z2vwJXeS13HrDXGLOml3Uo1Sui13JRwUJETgcWAoOMMZUishl4BqvH/q7d7uy2zsfAPGPMYz5ezwAjjTFF9vSLQLEx5i4ROQuYDyQbY1p6qGc8sNAYkyYig4ASIMMYs7/bcjnAFiDXGFMnIm8Ay40xfzjijaGUD9pDV8HkamC+MabSnn7FbhsM7Ooe5rbBwLYjfL8K7zAXkXgReUZEdolIHbAYSLW/IQwGqruHOYAxphT4EviOiKQCc7C+YSjVp3RHjwoKIhIHXAo47DFtgBggFSgDhohIpI9Q3wMM7+Flm7CGSDoMBIq9prt/fb0FOA441Rizz+6hrwHEfp90EUk1xtT4eK+XgJ9g/Z/7yhhT0vNvq9SR0R66ChYXAS5gLDDe/hkDLLHn7QUeFJEEEYkVkWn2es8Bt4rIyWIZISJD7XlrgStFxCEis4Hph6ghCWvcvEZE0oF7O2YYY/YCHwJP2jtPo0TkTK913wYmAjdjjakr1ec00FWwuBp4wRiz2xizr+MHa6fkFcAFwAhgN1Yv+zIAY8w/gd9iDc/UYwVruv2aN9vr1QDft+cdzKNAHFCJNW7/Ubf5PwDagc1AOfBfHTOMMc3Am0AB8K/D/N2V6hXdKaqUn4jIPcAoY8xVh1xYqSOgY+hK+YE9RPNjrF68Uv0iYD30zMxMk5+fH5D3VsqfKioqKC4uJj09naFDhx56BaUOYtWqVZXGmCxf8wLWQ8/Pz2flypWBenullApKIrKrp3m6U1QppUKEBrpSSoUIDXSllAoRGuhKKRUiNNCVUipEaKArpVSI0EBXSqkQoYGuDk9bI6z5O+glI5Q65migK9/q98FqHxcFXPR7eOfnUDi//2vY+28o/KT/30epEKGBHq7W/ws+vB1q9viev/RP8O6N0FDRtb2lznrcv9P3esbAqhehYounrbHKej23+/BqfOYM+Pt3D28dpcKYBnqoc7XDwt9Dc7d7Lnz8K1j2NGzo4Uqu2xdZj/V7rcedX8DaVyE22ZquK4WmalhwP7S3wBePQNkGWPk8vHcz/OMqa531b8L8u6yfHYugeT8s/J1VV2+1NfV+2R2Lrff0ZcNbsG1h719LqSCjV1sMdRvfgc8fhOZqOO+PnvaOkGyqsoK6cD6c8UsYeILVKy9bb81vKLN63S9+25qe+EPrsaoI3v8v6/WNgS8ehl1fWWPsAJVbPescf7G9zjbrm8Gav8Gg8TD6vN79DjW7IHtM75Z96QL7PS8Bka7z/nmN9Xhfbc/rb/kIGss9v6dSQUQDPdQ127e4dLV52lxOaLVDranK6l1XboGkQZC9FjZ63eehfi/s+Nwz3VhlPZZvhFr7bm1fPGw9Fn0C0YmQNQYqNnm9n90b3/dvqLPvvNZ+iF53u9d9mffv7BroO7+ALR/CzPshwuF7/boSSMnzTLc2eJ6/fT3MegDi0rqus2e59c0CA6NmQ/EKaxz/rDsO/HBQ6hikgR7q2uwgi0oAZyt88EuYeI1nfmMVNNi36Fz2FJiOcW4BjDWO7ojxLF9rj7lXb+/6PhkjoaoQWusg9wLrA6LjtXZ/ZT3uWQ4xSdbzjqEcgFUvWQE/5T+t6fl3QbJXGO/fBZvnwddPwomXWmG+ZR5ERMLM38Cal61vBpOv83rNF60Pq1m/s97rlcs889a+DFnHwbSbrGljYN5t1joJWVBfavX0q7dbH4SFH1vbT6m+MvX63n9DPQwa6KGu3g5rt9MaF1/zMuxe5plfVwwtdm/deO20jE+3AhGsXvDJP4Xlz1hDLcm5np52h/FXWOPpAEkDISYZWuxx+47XqdjsWb62BMo3w8d3wrbPrLa4dMgebe1A9fbR7dbrtdZZvXNHNEQnwZePws4lULLKWm7kTM86i+3hpfVven4/byuft74xNJTD+CthxV+sD5Gr3oC1r0DJautbQcpg67lSQUADPdS0NsDbP4OTr4ER3/IMizRXW2EM0FpvPcamQNlG63nKEKjdDWPnQvpwGDULnp9lzbvmfSuAlz9j9aRPvBS+eRNSh1jjzY0VcML3PIEenwmOqK51nf4L68Nl3avW9LKnrBB1O63pzFHw3k0wdFrX9U66Eta9YoX55Ouscfqyb2Dun2HrR113gH72W+sxawxkjoBN7x0Y5qdcCym58Ol9sH+H1bbjc4iKh58vtbbJuf9zOFtcqWOGBnqweu9mK7im/Kxr+yd3W0G26yvInegZ2mjeD3vXWc87hlgyRkKJfZORrOOsQM8+Hs66vetrDhoP7c2e6cQBMOdBiM+weuPbF1nh3iE+AyK6BXpyLky/3Zq34jlwtljPz/6VdbTM+Cvh0RNg2wLrQyUm2QrZ2b+3foftC2HkLDjtRlj6Zxj9betDpCPQxQHr37CeX/gnGHwK7FgCpWusD7Os0dBYCafdYC2TkGWtk3Wc9Y1g2HQrzJUKYhrowWrVi9Zj90DvGL5oqux68k9TNdTs7rpsplegT/9vGHSSJ/AAfviONVwS4ei6AzE+AyZ43ec4Z0LX143PgLSh1lj01BusbwSjz4eoOJj1W9j6sTXe/qOPIH2YZ70xF1gBPeXnMGSKp/3ip60PgWHTrZ7/eX/wzOsY/vn51/DqZda4d0quNa/gDOvHF+/6v/eC72WUCjIa6MGo+wk6tSXwj+/DaTdZOxB9qdgC7Y1dmvZG5TGoYyJ9GJxzd9d1hp3leR6X7nmeMfyg5VWTxI/Kf8KTU2eTc+7NBx4hculL1ri0d5gDzLgLcibC4FO7ticNtOb5cvV7/OvNv7P6y1Ye+P4b1s7SpEG+l1UqxOmJRUFgwaYyCu78gOv+avemW73GhZc9A4+MtYYW3vgPwBw43DH8nM4wf8U5o7P57d2xnmXi0ml3uXG5DS63od3l+dBYWlTJ5EdWdU6/U57d5eWNMbQ6XSDWn9OSUsPamjh+XXoaiLB8RzUn3vcxMx5axGOfFsKA42HiDzrXv/ed9dz7znor4E+74YAPgDdXFXPa7xdQ3djW5T0BXGnD+OX2k3n56928vTuW4+aNoMXpqX3OY0v4/YebOBxVDa1Me/Az3lxVfFjrKRVoGuhB4JuSWuvcnaJKVn/0Ivxvfue8qrXvAbB/xMWeFQrO7LL+poiRANSaeB52fq+zfSkT+EP7ZSw68Q9sKW9k7D0fcdqDC5jx0CJG3/0RK3ZW43IbXlm+m/JmT8je+kEx5XWe48RveHUNx931EYWR1vusqbD+rHZUNuJ0ufnnyj3UtTjZXtHII59u7VKbMYYPvtnLkqLKLu1ut+G2f65j0gOfcss/11Fa28KqXdYx9Q+8v5GZjyzGGMP6Es+H273vbqDV6aZ4fzOfbCwj/44P2LS3jmc+73aIJfCdp5byoxdX+NzeSworKalptt63ptnnMkodi3TIJQjst3umrW1tJC57pMu8pH3LYOxcno25kdt5y2ocMhW2LWCZezS/a7+SUzduYkwUPOO8gEpSOLv1IebktbO9TljimktbdAHDdu2n3WUoq2vtfO3vPf0VYwclk5cW1+U9212G9aW1zEiOpaXdxScbywC4rP4XnBixnUUrreu/7Kxq4tuPf8GwLM8x3IkxkbS0u/jZy6vISozhl+eOorKhjcZWF61OF5EREWzaW8f5f/rigO3wTUktNU1tPPeFdXRK8f5mXl/puRZNbbN1AtO+2hZeWrqzy7ptTjfRkdYHjTGm88OhQ6vTRbQjAhHhC68Pl5e+2snVU/MP/EdR6iikxEWRENP38auBHgSq7EAvjPkhEd0uWxtt2iDzOErKIjil5Um+O7iO/45sR4CN7qGsMyPY7sphu8nhU/dEAHaYQTy5B8Dqfe6ubqL7xXDPHTuA+RvL2LSvjo1768hNjWPTzHfJycmDxzazrbyRGaNh5c79tDndXDQ+h7fXwlKZCLi58KQc8jPieWLRNraU1TMwOZZRA5NYvLWC0Xd/BEBSbCTnjLGGb5rbXRx310eMHZTMxr11B2yDKIewcHM5W8vqKchMYEdlI39ftpu/L9tNdGQEbV7DLEXl9UQ6ug7bjLrrQz67ZTrDshLZvK++s93tNizfWc0Nr6zhjJGZ/GhaAQs2lXHeCQNpaHXxzOfbffbwlToaD1w0jqumDO3z19VAPwbtq22hsq6JAdGNZA0YzP6mNhKjhQjxfQ3ypuThfPTZPtpI5YV96ZxSsZcZwJjxp+FYLdS74/nUfXLn8qMHJvGjaQVERAj/XLmH3dVNtDrdjB2UTGxUBC3tbh6/YgLflNTyp8+KWLy1gmkjMhhz8kkAZCZup6jcOgP16+1VOCKE/7loHHMn5DI4LY7Vu2o4e3Q2WUkxbCitY8Hmck4bnsEZozJZvNVz9cb6FievLO96tUdfYS4Cc8YN4t11pQA8+f2JXPCnL3jhS6unfvf5Y7n77fWdy9/33kaf22nN7hq+LKrk7nc2dLa9uHQn97+/EUeE8NaaEt5aU0J8tIObzhlJcmwUSworfL6WUkfj5KFph17oCGigH2MaW53MeGgR97mfZFzk56y7ejPJtVv5XlYrVPle546lbtqc7s5e9Y++zuaHeQ9zz0XX8NaUekprWvjZy56dmpmJMVx6ymAANpTW8o8Ve6hrbmdSfjq3zxmN222IjXJwSn46k4amsXhrBTGRnmumDMtKpKjCCvSOHnNSbBRnH2f1tkdkJ3UuO7kgnQWby4mIEIakxx9Q++KtFURGCE6358Pq0kl5fLyhrHMIZWByLL+YOYp315VyQm4KYwYlM3pQEutL6hiYHMuYgUkHvC7ABSflkBjj4FX7Q+Obklr+vqzrUUDzN1rH5C/45XT27G+iurGN43NSGJGdCMBlpwxBqWChO0UDyO02bLOD0RhDYVk9y3dU09bWyqWR1gWxvn7jMZ6qv5F7q249YP2bE/8fP464n3n7Upk4JJXHLp/AuNxkBqXEcdM1VxEZ6eDEvFRmjxvYZT2XV3iOGZRMU5uL0toWRmYnkpsax2Cv4J02IhOA47xCc2R2Ilv31VNYVs/mffUMz+r5OienDbfWPz4nmaEZ1nLfmZjHZ7dMZ8wg61K8U4dndFnn4gl5XG5/4AAMTo+nIDOBr+88h5d/bB3SeP6JOQDERzvITIzBl2nDMzgl33O45YtLd9LuMnxw0+ncOGMEAF9vr+a8EwaSn5nAGSOzmDs+tzPMlQo2GuiBsn8Xry1exzkPfc66PTXM+2YfMx9ZzF1vr2dalOfmEBc3vNpltW2nP8QmtxV2aaNOY0HTCJxuw7VnDCMu2sHLPz6Vj39x5gEhNyzTE7pnHZfV+fyEXM/ZkZML0unu5KFpfHbLdK6cPKTLcvWtTmY+spjd1U0HDcAT8lL47JbpXD01n8zEGBbeehZ/+O6JDMtK5PQRGXY9nsMg46IcHJ+bzH/PHs2iW89iRHZiZw98YEosKfHWIZnfPdm6eNe3xg4gM8nzu462l11wy3QunTT4gO0wbUQGx+ekdPkQGZerZ4iq0KBDLv7Q1mRdfdA+o7K8voXsx07k/MgMfsWf2LZpNTGVG0lgICU1cOOAfVALRhxkU0OtiSdFrMvNDj9lNlcXjWDdzn38wiukO0IpNT7aZwnv33Q6bU43ze0uBiR5jj8f6RXGE4b4HtcbltU1sDt63R3yMw5+JULv9Qu8aj5nzACe+2IHpxaks/zX55AUE0VdSzvJsVZo52cm8M+fTiU26sBL5GYmxrDsV+eQnhBNZIS1A/TcsQN45LLxtLvcndshPaHr9vifueMAyPIK+vF5qQetX6lgoYHuD2//zLoRxO07IS6Ni//8JV8Cyc4qRkgx5399L9HuZhojp/MvZnB+ZhnIUMzAk5DN7/JX17ncGGlfozwhm2d/Mog2p5t1e6xjsNMTog84tLC7+OhI4qOhe3RFOiLITY0jMSay87C+Q8lKiiE3NY4S+xjtkwYfWSBOGZbBil9/q0svOi66a3inJfj+gAIYkOz5YFp377nERzuIcnT9HQamWMvcPns0V0we3Bn0GV7v6eubiVLBSAPdH3bZ1wNvroFdX9FSWwN2Fl3j+BiXgbKIAVwa+TmX8jlsA8bOJeKSv/DpsjU8/F6FJ9Ajo4kBYiIdnD4yk6V3zCAxNhI5ihswfHbrdITDW3/BLdOJEKHN5SbxKI6n7Wn8+3ClxEX5bM9MjGHj/bOIi3J02Uap9vI/nDqUSIeOPKrQoIHuD/Yp8WyZBx//igejJnbOGiA17HJn81vzHzwd/SgJaQOt64YPOwsiY/jWtClsPtUF/5jV9S5AtpzUg/fMe8P7CJbe6hgG6W2vPpDiow/8M4+IEAp/O6dzuEapUKCB7g8dgV5tHTc9NcJznHSuVFLuTmZJ+yj+NWcRPzh1qHWXoY6bMWMH7vdf92vJ4aD78IxSwU7/ov2hI9AbrFPkE8VzHZSxEbuoxNqheUJeGkREdAlzpZTqLQ10f+gYu+1+H05blbECfHQPJ8gopVRvaKD7hR3oVdt8zq00Vg/d1+F5SinVWxro/aC+pZ3739tIc5uLJxcV4Wyxr0/i9FyK1UR6dmaeeNwIfnPh8f4uUykVYnSnaD94ctE2nv9yB2nxUTz8yWZ+GlNL96MCpeOelxi+PfUkGJkfiFKVUiGkVz10EZktIltEpEhE7vAxf4iILBSRNSLybxE5r+9LDR41TdZFpXZUNfJg5HM4fF0l8fhLrJs4g3VDZKWUOkqH7KGLiAN4ApgJFAMrRORdY4z3NUrvAl43xjwlImOBeUB+P9QbFJrbnACUVdVwWeSiLvPKTSqRY+aQftIVMHwGLP4D5IwPQJVKqVDTmx76ZKDIGLPdGNMGvAbM7baMATq6mSlAad+VGHya2lwA7CvZ2dlWaR/JUmIySb/8aYiKhbShMPcJiOybsyWVUuGtN4GeC3jfhaDYbvN2H3CViBRj9c5v9PVCInKdiKwUkZUVFaF744COQE91VQOwyT2Y36f9DwBjklt7XE8ppY5GX+0UvQJ40RjzkIhMBf4mIuOMMW7vhYwxzwLPAkyaNMn37XeC3Ion/oPL95Xy6+hS3nWdBsAv2q9nyrBTIOdRYu0rLiqlVF/rTaCXAIO9pvPsNm8/BmYDGGO+EpFYIBMo74sig8kpFf8C+3DyHLFuNnzZ2adw6fTjIEYPTVRK9Z/eDLmsAEaKSIGIRAOXA+92W2Y3cA6AiIzBupZg6I6p9KClubHLdIo04ZJI/mPmyf1yh2+llPJ2yEA3xjiBG4CPgU1YR7NsEJH7ReRCe7FbgGtFZB3wKnCNMSYkh1QOpnxPYefzamOdxt8Uk+k59V8ppfpRr7qNxph5WDs7vdvu8Xq+EZjWt6UFkZZaeHAI8TlndTa9EzmLs9uXkBSfE7i6lFJhRccB+kKttUshs3SRNXneU7zwUTwvt5/KH047gYye11RKqT6j13LpA421nn2/zSaa5EmXU+WMZZvJJSFPd4QqpfxDA/0oudyGB99Y0jkt6flIRAStTuuIzYwEPWlIKeUfGuhHaUlhBdLoOaAnNms4ALPGDQQOvOu8Ukr1Fw30o/TO2lIGRzV4GtLyAXj40pP46s4ZOPSelUopP9FAPwrGGJYUVjI2tc3TaAd6TKSDQSlHfwNnpZTqLQ30o7C1rIHKhlbyY7xOKLIDXSml/E0PWzwKXxRZp/ZnSS1EJ4G7HQaOC3BVSqlwpT30o/BlUSWjMyKJ3l8I4y6BX+2FlLxAl6WUClMa6Eeo3eVm2fYqrk1fC611cNLlEKGbUykVOJpAR+jzLRU0trk4p+lDyBwFQ6YGuiSlVJjTQD9CLy/bxZTEMlKr1sDEq/UCXEqpgNNAPwJvripm0ZYKbhqy02o44XsBrUcppUCPcjksf1pQyPghqby1poQR2YlMiSuGlMGQNCDQpSmllAZ6b7U53Tz0ydbO6Usm5BKxdy0MOimAVSmllIcOufRSSU1zl+kx6Qaqt0HO+ABVpJRSXWmg99Lu6qYu0xNli/Ukb3IAqlFKqQNpoPfSHjvQU+KiABhetwIiY2HwqYEsSymlOmmg99Ke6iaiIyN474bTufmckaTsW2odex4VG+jSlFIK0EDvFWMMa/fUMDgtjiEZ8fxiSgpSvhGGnRXo0pRSqpMe5dILzy7eznf3/I5TBjmAs2DH59aMYWcFriillOpGA/0QCsvq+d+PNrM9ZjF03Jhox2KIS4OBJwa0NqWU8qaBfggLNpcTYZyehvtSrMchU/ViXEqpY4om0iF8WVTJ9MzGA2ck5/q/GKWUOggN9IMwxrBiZzUzs2sOnJmiga6UOrZooB9EU5uLlnY3+ew9cGay3shCKXVs0UA/iMY2a+w8ya09dKXUsU8DvQfNbS7Wl9QCkOj0EejJOX6uSCmlDk6PcunBxU9+yeZ99YySPSS17IWs0VCx2bNAyuDAFaeUUj5oD92HhlYnm/fVI7iZH3M76ZUrPD3yMRfCrUWQkBnYIpVSqhvtofuwbHsVAPG0ehrjM+FXpeCIAYduNqXUsUeTyYd9dS0AxNPiaYzPgOiEAFWklFKHpkMuPpTVWT3zBPEK9ISMAFWjlFK9o4HuQ0W9FeQJ3kMujugAVaOUUr2jge5DeV0rybGRXYdcWhsCV5BSSvWCBroPZfUtjByQ1HXI5aTLA1eQUkr1gga6D+V1rQzLTPD00P/zK8gYHtiilFLqEDTQu3G5De0NVXy/6nEyxTpTVI9uUUoFAz1ssZvSmmaucXzI+H1vkRlpnzwUnRjYopRSqhd61UMXkdkiskVEikTkDh/zHxGRtfbPVhHxcfGT4FBU3kC7sT7n8qTSatQeulIqCByyhy4iDuAJYCZQDKwQkXeNMRs7ljHG/MJr+RuBCf1Qa7/aWdnIAx9sZGxOClHiuUORWxxERMYEsDKllOqd3vTQJwNFxpjtxpg24DVg7kGWvwJ4tS+K86eHP9nKp5vKeXJhEblRnkMUI6ITQSSAlSmlVO/0JtBzgT1e08V22wFEZChQAHzWw/zrRGSliKysqKjwtUjAdGS2020YEu11y7mouMAUpJRSh6mvj3K5HHjDGOPyNdMY86wxZpIxZlJWVlYfv/XR2VvjOeZ8cLTXSUQN+wJQjVJKHb7eBHoJ4H3x7zy7zZfLCcLhFmMMRRWeEM+KqIMhUwNYkVJKHb7eBPoKYKSIFIhINFZov9t9IREZDaQBX/Vtif3r8QWFTP7dAqob27jy1CE884OTiWyuhEHjA12aUkodlkMe5WKMcYrIDcDHgAN43hizQUTuB1YaYzrC/XLgNWOM6b9y+94/Vuyhor6V7KQY/utbI8mOcUFbAyRmwXWLQByBLlEppXqlVycWGWPmAfO6td3Tbfq+vivLf1LiomhodfL6T6eSnRQLe9dZM9KHQU7QHX2plApjYX/qf3l9K+edMJD8TPvkocpC6zFzVOCKUkqpIxDWge50ualqbCUrKdbTWLEFJAIyRgSuMKWUOgJhHeiVDW0YAwOSvc4ErdwCafmgZ4cqpYJMWAd6uX1nomzvHnr1DkjXS+UqpYJPeAe6fe/Q7CSv3nhbI8SmBKgipZQ6cmEd6BtK6wDITfM6vb+9WU/3V0oFpbANdJfb8PrKPUwbkUFmolcPvb0RouIDV5hSSh2hsLzBRVF5PUu3VVFS08yts7odntjeDNEa6Eqp4BOWgf6thxd3Pj95SLpnhssJrjbtoSulglLYDrl0GJzuNV7ubLYedQxdKRWEwjLQk2M9X0zE++YV7RroSqngFZaBnhhjBfr/fueErjPam6xHHXJRSgWhsAz0uhYnP5pWwGWnDOk6o60j0LWHrpQKPmEX6E6Xm4ZWJylxUQfO7BxySfBvUUop1QfCLtDrW5wAJMf5OMCnXXvoSqngFXaBXtfSDkBy7MF66DqGrpQKPmF3HHpds9VD7zLk8sm91iVzv3jYmtYeulIqCIVdoNc22z1070D/8tGuC2mgK6WCUPgOuXSMobucBy4UrTtFlVLBJ/wCvbnbGHpT5YELaQ9dKRWEwi7QO4ZcOsfQGyusx4LpnoUiNdCVUsEn7AK9rqUdR4QQH+2wGhrKrcez7vAs5Ai7XQtKqRAQfoHe7CQ5NtJzDZdGe8glIRtikgNXmFJKHaWw64rWtbR3PWSx0e6hJ2TCDSugZk9gClNKqaMUdoFe29ze9ZDFhnJwRFv3EZVUSBoYuOKUUuoohNWQS3l9CxX1rZ4jXJxtsHMJJOeA92V0lVIqCIVNoLvdhtmPLmFDaZ3nGPT1b0LpGphxd2CLU0qpPhA2gV7R0Ep1Yxvgdchi5VaIiILjLw5gZUop1TfCJtB3Vzd1Pu8cctm/E1IHQ4QjMEUppVQfCp9Ar/IK9DivQE/LD0g9SinV18In0L166MYY64kGulIqhIRNoO/xCvSapnZoqYXmag10pVTICJtA31bRQE5KLADfGVQJn95nzcgaE7iilFKqD4XFiUXtLjeb9tVz9dSh/PrbY+HREwphEiYAABHpSURBVKFmlzVz+NmBLU4ppfpIWPTQt5bV0+Z0My43xWpwu6zHbz8EDh+3olNKqSAUFoG+bk8tACfkpoAx0FQFU66HU34S4MqUUqrvhHyg729s4+FPtjI8K4H8jAQrzJ3NkDok0KUppVSfCvkx9GU7qqlsaOXPV04gIkI8Y+epgwNbmFJK9bGQ76F3HK44ZpB9rfOOy+OmaKArpUJLrwJdRGaLyBYRKRKRO3pY5lIR2SgiG0Tklb4t88g8t2Q7v523ieTYSM/1W2rtQNchF6VUiDnkkIuIOIAngJlAMbBCRN41xmz0WmYkcCcwzRizX0Sy+6vgw/HAB5sAGJwe72ms2W3dmSguNUBVKaVU/+hND30yUGSM2W6MaQNeA+Z2W+Za4AljzH4AY0x535bZew+8v5E/f1boOb0f6zj0TjV7dLhFKRWSerNTNBfwvi9bMXBqt2VGAYjIl4ADuM8Y81H3FxKR64DrAIYM6Z8hj+e+2AHAJRPzOtvio71+zdo9OtyilApJfbVTNBIYCZwFXAH8RUQOGNMwxjxrjJlkjJmUlZXVR2/t24tLdwJw5qgs/nTFBM+Mmt16hItSKiT1JtBLAO8EzLPbvBUD7xpj2o0xO4CtWAHvd5ER1q3kFm62Rn1+OXOUZwy9uQZa63TIRSkVknoT6CuAkSJSICLRwOXAu92WeRurd46IZGINwWzvwzp7pdXpwum2xs4LyxsAyEmN9SzQeYSLBrpSKvQcMtCNMU7gBuBjYBPwujFmg4jcLyIX2ot9DFSJyEZgIXCbMaaqv4ruSWOrq8t0tCOCzIQYT8P+ndajXjJXKRWCenWmqDFmHjCvW9s9Xs8N8Ev7J2AaWpwAxEU5aG53cdLgFOvs0A4a6EqpEBZSZ4o2tFqBnpsWB+C5uqKrHRqrrECPTYG4tABVqJRS/SckA/1EO8jPHGUfSfPxr+GPw6Bii/bOlVIhK6QuztVoB/pVU4dy7ZnDPNdv2WKPFu1cAmMu7GFtpZQKbiHVQ69vdZInFaS5azxhDpA61PM8Y4T/C1NKKT8IqUBvbHXyRczNDH31zK4zouI8z8dd4t+ilFLKT0Iq0JuarEvlRrTWdZ3RvN96TBoEA0/wc1VKKeUfIRXoUTU9nMvUvN8aO795nX8LUkopPwqpQI+v2+aZ2L8LNtontDZXQ2I2RMb4XlEppUJASAV6SqN1pUXiM2HFX+D1H8DaV61ruMSlB7Y4pZTqZyF12GJ8q30ZduOC1nrr+ds/sx71ZCKlVIgLqR66OFutJ60N0NYICV6X6I3XHrpSKrSFVKDjbvc8NtdA4gAYPMVqiwipLyNKKXWAkAr0CFebZ6KxHKLi4ZJnoOBMyD8jcIUppZQfhFagu70Cvb4MouOta7dc/R4kDQhYXUop5Q8hFejSMeQC0FAG0YmBK0YppfwspALd4R3oGGvIRSmlwkRoBbpppy3C65Zz0RroSqnwEWKB7qQ5MsXToEMuSqkwEjKB7nYbIk07rVFega5DLkqpMBIygd7qdBOFk7ZorzNCdchFKRVGQibQm9tdROOkNdbr7FAdclFKhZGQCfSmNifR0g6RseCIthp1yEUpFUZCJtBb2l1E4SQiKhoSsq1GHXJRSoWRkAn05jY30TiJiIyBhEyrUYdclFJhJHQC3e6hO6JiPFdZ9L62i1JKhbjQCfQ2JzFiD7mMPNdq9L58rlJKhbiQuaZsS2sLAJFRsTD5Whg2HbKOC3BVSinlP6HTQ29uBiAyKgZENMyVUmEnZAK9tqERgPj4uABXopRSgRE6gV5vBXpMjAa6Uio8hU6g2z10iYwOcCVKKRUYIRPodY1N1hOHBrpSKjyFTKA3Nlo9dA10pVS4CplAb2iyjnLRQFdKhauQCHS329BkH7aIjqErpcJUSAR6bXO7536i2kNXSoWpkDhTdNO+OqLEaU1ooCsV0trb2ykuLqalpSXQpfSr2NhY8vLyiIqK6vU6IRHo7Z8/wizHFmtCA12pkFZcXExSUhL5+fmISKDL6RfGGKqqqiguLqagoKDX64VEoE/f/Wdw2BMa6EqFtJaWlpAOcwARISMjg4qKisNar1dj6CIyW0S2iEiRiNzhY/41IlIhImvtn58cVhVHwbQ1dW3QQFcq5IVymHc4kt/xkIEuIg7gCWAOMBa4QkTG+lj0H8aY8fbPc4ddSW+tfAEePh5c1ph5fVVJ1/lxqf321kopdSzrTQ99MlBkjNlujGkDXgPm9m9ZB2FcUFcMTZUA7N+3yzNPHBCfGaDClFLhoKamhieffPKw1zvvvPOoqanph4o8ehPoucAer+liu62774jIv0XkDREZ7OuFROQ6EVkpIisPd2yoU8f9QhvKAaiv8OqhJ2ZDREgciamUOkb1FOhOp/Og682bN4/U1P4dQeirnaLvAa8aY1pF5KfAS8CM7gsZY54FngWYNGmSOaJ3SrQDvdEK9Jb9xZ550QlH9JJKqeD0m/c2sLG0rk9fc2xOMvdecHyP8++44w62bdvG+PHjiYqKIjY2lrS0NDZv3szWrVu56KKL2LNnDy0tLdx8881cd911AOTn57Ny5UoaGhqYM2cOp59+OkuXLiU3N5d33nmHuLijv1Jsb7qzJYB3jzvPbutkjKkyxrTak88BJx91ZT3puK1cozXk0lJd6pkX0fvjNZVS6kg8+OCDDB8+nLVr1/LHP/6R1atX89hjj7F161YAnn/+eVatWsXKlSt5/PHHqaqqOuA1CgsLuf7669mwYQOpqam8+eabfVJbb3roK4CRIlKAFeSXA1d6LyAig4wxe+3JC4FNfVKdDy0x6cQCNJTz6cYyGku3ex2yGBJHYSqleulgPWl/mTx5cpdjxR9//HHeeustAPbs2UNhYSEZGRld1ikoKGD8+PEAnHzyyezcubNPajlkD90Y4wRuAD7GCurXjTEbROR+EbnQXuwmEdkgIuuAm4Br+qQ6H/5veSUtJgpnfRnbK+o5JWIzrXEDrJnaQ1dK+VlCgmeod9GiRXz66ad89dVXrFu3jgkTJvg8ozUmJqbzucPhOOT4e2/1qktrjJkHzOvWdo/X8zuBO/ukokPISo6lkhTSasqIat1OjlRjTroevn4ChkzxRwlKqTCWlJREfX29z3m1tbWkpaURHx/P5s2b+frrr/1aW9CNUWQnxVBpkkmqKyOzeTUAMulHMO4SGHRSgKtTSoW6jIwMpk2bxrhx44iLi2PAgAGd82bPns3TTz/NmDFjOO6445gyxb+dzKAL9AHJsZSaFAqaKomN2osbISItX8fPlVJ+88orr/hsj4mJ4cMPP/Q5r2OcPDMzk/Xr13e233rrrX1WV9AdtJ2dFEM9cUhrPfGtFdRIioa5UkoRhIGeFh9NM3E4nA0ktFWy35Fx6JWUUioMBF3XNiJCcEUnEe1sJNlUUhupga6UUhCEPXQAYpKIMm1kuMppjNJAV0opCNJAj01IBiDF1NMUkxXgapRS6tgQlIGe43WYUFtsdgArUUqpY0dQBvrwvIGdz9viNdCVUv5zpJfPBXj00Udpamo69IJHKCgDPTvTc81zd+KAgyyplFJ961gO9KA7ygVAYpI8E0mDAleIUiqwPrwD9n3Tt6858ASY82CPs70vnztz5kyys7N5/fXXaW1t5eKLL+Y3v/kNjY2NXHrppRQXF+Nyubj77rspKyujtLSUs88+m8zMTBYuXNi3dROkgY4d6G4jOJJ0yEUp5T8PPvgg69evZ+3atcyfP5833niD5cuXY4zhwgsvZPHixVRUVJCTk8MHH3wAWNd4SUlJ4eGHH2bhwoVkZvbPndWCM9CjEwFocKQwsUADXamwdZCetD/Mnz+f+fPnM2HCBAAaGhooLCzkjDPO4JZbbuH222/n/PPP54wzzvBLPcEZ6HYPPTlrMMkZepcipVRgGGO48847+elPf3rAvNWrVzNv3jzuuusuzjnnHO655x4fr9C3gnKnaEegk6Q7RJVS/uV9+dxZs2bx/PPP09DQAEBJSQnl5eWUlpYSHx/PVVddxW233cbq1asPWLc/BGcPPcIBUfGQOPDQyyqlVB/yvnzunDlzuPLKK5k6dSoAiYmJvPzyyxQVFXHbbbcRERFBVFQUTz31FADXXXcds2fPJicnp192iooxR3av5qM1adIks3LlyiN/geV/gZyJkNd/ty9VSh17Nm3axJgxYwJdhl/4+l1FZJUxZpKv5YOzhw4w+dpAV6CUUseU4BxDV0opdQANdKVU0AnUULE/HcnvqIGulAoqsbGxVFVVhXSoG2OoqqoiNjb2sNYL3jF0pVRYysvLo7i4mIqKikCX0q9iY2PJy8s7rHU00JVSQSUqKoqCgoJAl3FM0iEXpZQKERroSikVIjTQlVIqRATsTFERqQB2HeHqmUBlH5bTV47VuuDYrU3rOjxa1+EJxbqGGmN83kw5YIF+NERkZU+nvgbSsVoXHLu1aV2HR+s6POFWlw65KKVUiNBAV0qpEBGsgf5soAvowbFaFxy7tWldh0frOjxhVVdQjqErpZQ6ULD20JVSSnWjga6UUiEi6AJdRGaLyBYRKRKROwJcy04R+UZE1orISrstXUQ+EZFC+zHND3U8LyLlIrLeq81nHWJ53N5+/xaRiX6u6z4RKbG32VoROc9r3p12XVtEZFY/1jVYRBaKyEYR2SAiN9vtAd1mB6kroNtMRGJFZLmIrLPr+o3dXiAiy+z3/4eIRNvtMfZ0kT0/vz/qOkRtL4rIDq9tNt5u9+ffv0NE1ojI+/Z0/28vY0zQ/AAOYBswDIgG1gFjA1jPTiCzW9sfgDvs53cA/+uHOs4EJgLrD1UHcB7wISDAFGCZn+u6D7jVx7Jj7X/PGKDA/nd29FNdg4CJ9vMkYKv9/gHdZgepK6DbzP69E+3nUcAyezu8Dlxutz8N/Kf9/OfA0/bzy4F/9OPfWE+1vQh818fy/vz7/yXwCvC+Pd3v2yvYeuiTgSJjzHZjTBvwGjA3wDV1Nxd4yX7+EnBRf7+hMWYxUN3LOuYCfzWWr4FUERnkx7p6Mhd4zRjTaozZARRh/Xv3R117jTGr7ef1wCYglwBvs4PU1RO/bDP7926wJ6PsHwPMAN6w27tvr47t+AZwjohIX9d1iNp64pd/SxHJA74NPGdPC37YXsEW6LnAHq/pYg7+B9/fDDBfRFaJyHV22wBjzF77+T5gQGBK67GOY2Eb3mB/3X3ea0gqIHXZX28nYPXsjplt1q0uCPA2s4cP1gLlwCdY3wZqjDFOH+/dWZc9vxbI6I+6fNVmjOnYZr+1t9kjIhLTvTYfdfelR4H/Btz2dAZ+2F7BFujHmtONMROBOcD1InKm90xjfYcK+HGhx0odtqeA4cB4YC/wUKAKEZFE4E3gv4wxdd7zArnNfNQV8G1mjHEZY8YDeVjfAkb7u4aedK9NRMYBd2LVeAqQDtzur3pE5Hyg3Bizyl/v2SHYAr0EGOw1nWe3BYQxpsR+LAfewvpDL+v4Cmc/lgeovJ7qCOg2NMaU2f8B3cBf8AwR+LUuEYnCCs2/G2P+ZTcHfJv5qutY2WZ2LTXAQmAq1nBFx01yvN+7sy57fgpQ1Z91datttj18ZYwxrcAL+HebTQMuFJGdWMPCM4DH8MP2CrZAXwGMtPcWR2PtQHg3EIWISIKIJHU8B84F1tv1XG0vdjXwTiDqO0gd7wI/tPf2TwFqvYYZ+l238cqLsbZZR12X23v8C4CRwPJ+qkGA/wM2GWMe9poV0G3WU12B3mYikiUiqfbzOGAm1vj+QuC79mLdt1fHdvwu8Jn9jafP9VDbZq8PZsEaq/beZv36b2mMudMYk2eMycfKqM+MMd/HH9urr/bo+usHay/1VqwxvF8HsI5hWEcYrAM2dNSCNfa1ACgEPgXS/VDLq1hfxduxxuZ+3FMdWHv3n7C33zfAJD/X9Tf7ff9t/yEP8lr+13ZdW4A5/VjX6VjDKf8G1to/5wV6mx2kroBuM+BEYI39/uuBe7z+DyzH2hn7TyDGbo+1p4vs+cP68d+yp9o+s7fZeuBlPEfC+O3v336/s/Ac5dLv20tP/VdKqRARbEMuSimleqCBrpRSIUIDXSmlQoQGulJKhQgNdKWUChEa6EopFSI00JVSKkT8fxWBwa/1nEDYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaIeoW0vAeDZ"
      },
      "source": [
        "**Para você:**\r\n",
        "\r\n",
        "Tente executar este código com e sem a **tabela (programação) da taxa de aprendizado** e descrever o efeito que a programação (tabela) da taxa de aprendizado tem nas curvas de aprendizado durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SA5QzNqJSPY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}